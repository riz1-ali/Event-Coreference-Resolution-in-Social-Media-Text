{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dataloader import dataset, collate_fn\n",
    "import pickle\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torchnlp.nn import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = []\n",
    "file_path = './FinalDataset.csv'\n",
    "with open(file_path, 'r') as f:\n",
    "    for i in f:\n",
    "        j = i.strip('\\n').split('\\t')\n",
    "        data_.append(j[-1].lower())\n",
    "\n",
    "tweet_pairs, distance_vectors = [], []\n",
    "with open('./tweet_pairs.pkl', 'rb') as f:\n",
    "    tweet_pairs = pickle.load(f)\n",
    "with open('./distance_vectors.pkl', 'rb') as f:\n",
    "    distance_vectors = pickle.load(f)\n",
    "with open(\"./trigger_word_pos.pkl\", 'rb') as f:\n",
    "    trigger_word_pos = pickle.load(f)\n",
    "    \n",
    "tweet_pair_data = [[data_[i[0]], data_[i[1]]] for i in tweet_pairs]\n",
    "distance_vector_data = [[distance_vectors[i[0]],distance_vectors[i[1]]] for i in tweet_pairs]\n",
    "trigger_word_pos_data = [[trigger_word_pos[i[0]], trigger_word_pos[i[1]]] for i in tweet_pairs]\n",
    "labels_data = [i[2] for i in tweet_pairs]\n",
    "\n",
    "dataset_ = dataset(tweet_pair_data, distance_vector_data, trigger_word_pos_data, labels_data)\n",
    "loader = data.DataLoader(dataset_, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,max_dist):\n",
    "        super().__init__()\n",
    "        with open(\"./vocab.pkl\", \"rb\") as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "        pre_trained_emb = torch.Tensor(self.vocab.vectors)\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(pre_trained_emb)\n",
    "        self.distance_embedding = nn.Embedding(64, 14)\n",
    "        self.lstm = nn.LSTM(114, 64, batch_first=True, bidirectional=True)\n",
    "        self.selective = nn.Linear(128, 1)\n",
    "        self.attention_linear = nn.Linear(128, 128)\n",
    "        self.attention = Attention(128)\n",
    "        self.final = nn.Linear(512, 1)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(1 * 2, batch_size, 64)).to(device),\n",
    "                Variable(torch.zeros(1 * 2, batch_size, 64)).to(device))\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, tweet1, tweet2, dist1, dist2, pos1, pos2):\n",
    "        \n",
    "        batch_size = tweet1.shape[0]\n",
    "        seq_len = tweet1.shape[1]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "        \n",
    "        tweet1_embedding = self.word_embedding(tweet1.long())\n",
    "        tweet2_embedding = self.word_embedding(tweet2.long())\n",
    "        dist1_embedding = self.distance_embedding(dist1.long())\n",
    "        dist2_embedding = self.distance_embedding(dist2.long())\n",
    "        \n",
    "        tweet1 = torch.cat([tweet1_embedding, dist1_embedding], dim=2)        \n",
    "        tweet2 = torch.cat([tweet2_embedding, dist2_embedding], dim=2)\n",
    "        \n",
    "        # Tweet1\n",
    "        output1, (h_n, c_n) = self.lstm(tweet1, (h_0, c_0))\n",
    "        output_1 = output1.view(batch_size, seq_len, 2, 64)\n",
    "        indices = torch.Tensor(list(range(batch_size))).long()\n",
    "        \n",
    "        ment_part1 = output_1[indices, pos1[:, 1].long(), 0, :]\n",
    "        ment_part2 = output_1[indices, 0, 1, :]\n",
    "        \n",
    "        mention_feature1 = torch.cat([ment_part1, ment_part2], dim=1)\n",
    "        Rc1 = output1 * mention_feature1.view(batch_size, 1, -1)\n",
    "        alpha1 = torch.tanh(self.selective(Rc1))\n",
    "        select1 = alpha1 * output1\n",
    "        t = self.attention(mention_feature1.view(batch_size, 1, -1), select1)\n",
    "        Vem1 = torch.cat([mention_feature1, t[0].view(batch_size, -1)], dim=1)\n",
    "#         ex = torch.tanh(self.attention_linear(select1))\n",
    "#         print(ex.shape)\n",
    "        \n",
    "        # Tweet2\n",
    "        batch_size = tweet2.shape[0]\n",
    "        seq_len = tweet2.shape[1]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "        output2, (h_n, c_n) = self.lstm(tweet2, (h_0, c_0))\n",
    "        output_2 = output2.view(batch_size, seq_len, 2, 64)\n",
    "        indices = torch.Tensor(list(range(batch_size))).long()\n",
    "        \n",
    "        ment_part1 = output_2[indices, pos2[:, 1].long(), 0, :]\n",
    "        ment_part2 = output_2[indices, 0, 1, :]\n",
    "        \n",
    "        mention_feature2 = torch.cat([ment_part1, ment_part2], dim=1)\n",
    "        Rc2 = output2 * mention_feature2.view(batch_size, 1, -1)\n",
    "        alpha2 = torch.tanh(self.selective(Rc2))\n",
    "        select2 = alpha2 * output2\n",
    "        t = self.attention(mention_feature2.view(batch_size, 1, -1), select2)\n",
    "        Vem2 = torch.cat([mention_feature2, t[0].view(batch_size, -1)], dim=1)\n",
    "        \n",
    "        final = torch.cat([Vem1, Vem2], dim=1)\n",
    "        return self.final(final)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.4703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.3969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.3360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.2645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.1933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.1266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(6.5138e-05, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(3.7511e-05, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(2.1294e-05, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(1.2327e-05, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(7.2662e-06, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(4.2952e-06, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(2.6114e-06, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(1.5628e-06, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(9.5926e-07, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(6.1281e-07, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(3.4459e-07, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.2443, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(1.3039e-07, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(1.0617e-07, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(1.4901e-08, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(3.7253e-09, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-664633b5d788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IRE_Project/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/IRE_Project/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for tweet1, tweet2, dist1, dist2, pos1, pos2, label in loader:\n",
    "    tweet1 = tweet1.to(device)\n",
    "    tweet2 = tweet2.to(device)\n",
    "    dist1 = dist1.to(device)\n",
    "    dist2 = dist2.to(device)\n",
    "    pos1 = pos1.to(device)\n",
    "    pos2 = pos2.to(device)\n",
    "    label = label.to(device)\n",
    "    prediction = model(tweet1, tweet2, dist1, dist2, pos1, pos2)\n",
    "    loss = criterion(prediction.squeeze(), label.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
